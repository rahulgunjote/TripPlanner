default_platform(:ios)

platform :ios do
  # Variables
  SCHEME = "TripPlanner"
  PROJECT = "TripPlanner.xcodeproj"
  
  # Detect CI environment
  IS_CI = ENV['CI'] == 'true' || ENV['GITHUB_ACTIONS'] == 'true'
  
  # Configuration based on environment
  if IS_CI
    # CI Environment (GitHub Actions)
    SIMULATOR = "iPhone 16"
    IOS_VERSION = "18.5"  # Xcode 16.4 supports up to iOS 18.5.99
    XCODE_VERSION = "16.4"
  else
    # Local Development Environment
    SIMULATOR = "iPhone 17"
    IOS_VERSION = "26.0"
    XCODE_VERSION = nil # Skip Xcode version check locally
  end
  
  before_all do
    if IS_CI
      UI.important("🤖 Running in CI environment")
      UI.message("Simulator: #{SIMULATOR}")
      UI.message("iOS Version: #{IOS_VERSION}")
      ensure_xcode_version(version: XCODE_VERSION)
    else
      UI.important("💻 Running in local environment")
      UI.message("Simulator: #{SIMULATOR}")
      UI.message("iOS Version: #{IOS_VERSION}")
    end
  end

  desc "Build the project"
  lane :build do
    UI.message("🔨 Building TripPlanner...")
    
    destination = "platform=iOS Simulator,name=#{SIMULATOR},OS=#{IOS_VERSION}"
    UI.message("Building for destination: #{destination}")
    
    xcodebuild(
      scheme: SCHEME,
      project: PROJECT,
      configuration: "Debug",
      destination: destination,
      clean: true,
      build: true,
      xcargs: "-quiet"
    )
    
    UI.success("✅ Build completed successfully!")
  end

  desc "Run all tests"
  lane :test do
    UI.message("🧪 Running all tests...")
    
    destination = "platform=iOS Simulator,name=#{SIMULATOR},OS=#{IOS_VERSION}"
    UI.message("Testing on destination: #{destination}")
    
    scan(
      scheme: SCHEME,
      project: PROJECT,
      destination: destination,
      clean: true,
      code_coverage: true,
      only_testing: ["TripPlannerTests"],
      output_directory: "./fastlane/test_output",
      output_types: "html,junit",
      fail_build: true,
      reset_simulator: true
    )
    
    UI.success("✅ All tests passed!")
  end

  desc "Run unit tests only"
  lane :unit_tests do
    UI.message("🧪 Running unit tests...")
    
    destination = "platform=iOS Simulator,name=#{SIMULATOR},OS=#{IOS_VERSION}"
    UI.message("Testing on destination: #{destination}")
    
    scan(
      scheme: SCHEME,
      project: PROJECT,
      destination: destination,
      clean: true,
      code_coverage: true,
      only_testing: ["TripPlannerTests"],
      output_directory: "./fastlane/test_output/unit_tests",
      output_types: "html,junit",
      fail_build: true,
      reset_simulator: true
    )
    
    UI.success("✅ Unit tests passed!")
  end

  desc "Build for testing (used in CI)"
  lane :build_for_testing do
    UI.message("🔨 Building for testing...")
    
    destination = "platform=iOS Simulator,name=#{SIMULATOR},OS=#{IOS_VERSION}"
    UI.message("Building for destination: #{destination}")
    
    xcodebuild(
      scheme: SCHEME,
      project: PROJECT,
      configuration: "Debug",
      destination: destination,
      build_for_testing: true,
      xcargs: "-quiet"
    )
    
    UI.success("✅ Build for testing completed!")
  end

  desc "Generate code coverage report"
  lane :coverage do
    UI.message("📊 Generating code coverage report...")
    
    xcov(
      scheme: SCHEME,
      project: PROJECT,
      output_directory: "./fastlane/test_output/coverage",
      minimum_coverage_percentage: 60.0,
      ignore_file_path: "./.xcovignore"
    )
    
    UI.success("✅ Coverage report generated!")
  end

  desc "Lint Swift code"
  lane :lint do
    UI.message("🔍 Linting Swift code...")
    
    # Note: Requires SwiftLint to be installed
    # Install with: brew install swiftlint
    swiftlint(
      mode: :lint,
      config_file: ".swiftlint.yml",
      reporter: "html",
      output_file: "./fastlane/test_output/swiftlint.html",
      ignore_exit_status: false
    )
    
    UI.success("✅ Linting completed!")
  end

  desc "Run full CI pipeline"
  lane :ci do
    UI.message("🚀 Running full CI pipeline...")
    
    build
    unit_tests
    
    UI.success("✅ CI pipeline completed successfully!")
  end
  
  # MARK: - Launchable Integration
  
  desc "Record build for Launchable"
  lane :launchable_record_build do
    UI.message("📝 Recording build with Launchable...")
    
    sh("launchable record build --name '#{ENV['GITHUB_RUN_ID'] || Time.now.to_i}'")
    
    UI.success("✅ Build recorded!")
  end
  
  desc "Run full test suite and upload to Launchable (for training ML models)"
  lane :launchable_train do
    UI.header("📊 Running Full Test Suite for Launchable Training")
    
    # Record session start
    session_id = sh("launchable record session --build '#{ENV['GITHUB_RUN_ID'] || Time.now.to_i}'", log: false).strip
    ENV['LAUNCHABLE_SESSION_ID'] = session_id
    UI.message("Session ID: #{session_id}")
    
    # Run full test suite
    UI.message("🧪 Running all tests...")
    begin
      test  # Run full suite
    rescue => e
      UI.error("❌ Tests failed: #{e.message}")
      # Continue to upload results even if tests fail
    end
    
    # Record test results
    UI.message("📤 Uploading test results to Launchable for training...")
    
    # Find JUnit XML files
    junit_files = Dir.glob("./fastlane/test_output/**/*.junit").join(" ")
    
    if junit_files.empty?
      UI.important("⚠️ No JUnit files found to upload")
    else
      begin
        sh("launchable record tests --session #{session_id} file #{junit_files}")
        UI.success("✅ Test results uploaded to Launchable")
        UI.message("📈 This data will be used to train ML models for intelligent test selection")
      rescue => e
        UI.error("❌ Failed to upload test results: #{e.message}")
        raise e  # Fail the build if upload fails
      end
    end
    
    UI.success("✅ Training data uploaded successfully!")
  end
  
  desc "Run tests with Launchable intelligent subset (for PRs)"
  lane :launchable_subset_test do
    UI.header("🧠 Running Launchable Intelligent Test Selection")
    
    # Record session start
    session_id = sh("launchable record session --build '#{ENV['GITHUB_RUN_ID'] || Time.now.to_i}'", log: false).strip
    ENV['LAUNCHABLE_SESSION_ID'] = session_id
    UI.message("Session ID: #{session_id}")
    
    # Build for testing
    UI.message("🔨 Building for testing...")
    build_for_testing
    
    # Try to get intelligent subset from Launchable
    UI.message("🎯 Getting intelligent test subset from Launchable...")
    
    subset_tests = []
    use_launchable_subset = false
    
    begin
      # Step 1: Get list of all available tests from test files
      UI.message("📋 Discovering available tests from test files...")
      
      # Find all test files
      test_files = Dir.glob("./TripPlannerTests/**/*Tests.swift")
      all_tests = []
      
      # Parse test files to extract test methods
      test_files.each do |file|
        content = File.read(file)
        class_name = File.basename(file, ".swift")
        
        # Extract test method names (methods starting with 'test')
        content.scan(/func (test\w+)\(\)/).each do |match|
          test_method = match[0]
          # Format: ClassName/testMethodName
          all_tests << "TripPlannerTests/#{class_name}/#{test_method}"
        end
      end
      
      if all_tests.empty?
        UI.important("⚠️ No tests found in test files, using default subset")
        raise "No tests discovered"
      end
      
      UI.message("📊 Found #{all_tests.count} total tests across #{test_files.count} test files")
      
      # Step 2: Write test list to file for Launchable
      test_list_file = "./launchable_test_list.txt"
      File.write(test_list_file, all_tests.join("\n"))
      UI.message("📝 Test list written to #{test_list_file}")
      
      # Step 3: Get subset recommendation from Launchable
      UI.message("🧠 Requesting intelligent subset from Launchable ML...")
      subset_output_file = "./launchable_subset.txt"
      
      # Use launchable subset command (target 60% time reduction)
      subset_cmd = "launchable subset --session #{session_id} --target 40% file #{test_list_file} > #{subset_output_file}"
      sh(subset_cmd)
      
      # Step 4: Parse Launchable's recommendations
      subset_content = File.read(subset_output_file).strip
      
      if subset_content.empty?
        UI.important("⚠️ Launchable returned empty subset, using default")
        raise "Empty subset"
      end
      
      # Extract test identifiers from subset
      subset_tests_raw = subset_content.split("\n").reject(&:empty?)
      
      # Group tests by class for scan's only_testing parameter
      # Convert "TripPlannerTests/ClassName/testMethod" to "TripPlannerTests/ClassName/testMethod"
      subset_tests = subset_tests_raw.map { |t| t.strip }
      
      if subset_tests.empty?
        UI.important("⚠️ No valid tests in subset, using default")
        raise "Empty subset after parsing"
      end
      
      UI.success("✅ Launchable recommended #{subset_tests.count}/#{all_tests.count} tests (#{(subset_tests.count.to_f/all_tests.count*100).round}%)")
      UI.message("🎯 Selected tests:")
      subset_tests.first(10).each { |t| UI.message("   - #{t}") }
      UI.message("   ... and #{subset_tests.count - 10} more") if subset_tests.count > 10
      
      use_launchable_subset = true
      
    rescue => e
      UI.error("❌ Failed to get Launchable subset: #{e.message}")
      UI.important("⚠️ Falling back to full test suite")
      subset_tests = nil
      use_launchable_subset = false
    end
    
    # Step 5: Run the subset tests
    begin
      destination = "platform=iOS Simulator,name=#{SIMULATOR},OS=#{IOS_VERSION}"
      
      if use_launchable_subset && !subset_tests.nil? && !subset_tests.empty?
        UI.important("🎯 Running Launchable-recommended test subset (#{subset_tests.count} tests)")
        
        scan(
          scheme: SCHEME,
          project: PROJECT,
          destination: destination,
          only_testing: subset_tests,
          code_coverage: true,
          output_directory: "./fastlane/test_output",
          output_types: "html,junit",
          fail_build: true,
          reset_simulator: true
        )
        
        UI.success("✅ Intelligent subset tests completed successfully")
      else
        UI.important("🎯 Running full test suite (Launchable subset unavailable)")
        
        scan(
          scheme: SCHEME,
          project: PROJECT,
          destination: destination,
          clean: true,
          code_coverage: true,
          only_testing: ["TripPlannerTests"],
          output_directory: "./fastlane/test_output",
          output_types: "html,junit",
          fail_build: true,
          reset_simulator: true
        )
        
        UI.success("✅ Full test suite completed successfully")
      end
      
    rescue => e
      UI.error("❌ Test execution failed: #{e.message}")
      raise e
    end
    
    # Step 6: Upload test results to Launchable
    UI.message("📤 Uploading test results to Launchable...")
    
    # Find JUnit XML files
    junit_files = Dir.glob("./fastlane/test_output/**/*.junit").join(" ")
    
    if junit_files.empty?
      UI.important("⚠️ No JUnit files found to upload")
    else
      begin
        sh("launchable record tests --session #{session_id} file #{junit_files}")
        UI.success("✅ Test results uploaded to Launchable")
        
        if use_launchable_subset
          UI.message("📈 Launchable ML model will learn from these results")
        end
      rescue => e
        UI.error("❌ Failed to upload test results: #{e.message}")
        UI.important("Continuing anyway...")
      end
    end
    
    # Cleanup temporary files
    sh("rm -f ./launchable_test_list.txt ./launchable_subset.txt", log: false) rescue nil
    
    UI.success("✅ Launchable subset testing completed!")
  end
  
  desc "Run tests with Launchable (with fallback)"
  lane :launchable_test do
    begin
      launchable_subset_test
    rescue => e
      UI.error("❌ Launchable test failed: #{e.message}")
      UI.important("⚠️ Falling back to full test suite...")
      test
    end
  end
  
  desc "Verify Launchable setup"
  lane :launchable_verify do
    UI.header("🔍 Verifying Launchable Setup")
    
    # Check if launchable is installed
    begin
      version = sh("launchable --version", log: false).strip
      UI.success("✅ Launchable installed: #{version}")
    rescue
      UI.error("❌ Launchable not installed")
      UI.message("Install with: pip install launchable")
      return
    end
    
    # Check if token is set
    if ENV['LAUNCHABLE_TOKEN'].nil? || ENV['LAUNCHABLE_TOKEN'].empty?
      UI.error("❌ LAUNCHABLE_TOKEN not set")
      UI.message("Set with: export LAUNCHABLE_TOKEN=your_token")
    else
      UI.success("✅ LAUNCHABLE_TOKEN is set")
    end
    
    # Verify organization
    begin
      sh("launchable verify")
      UI.success("✅ Launchable authentication successful")
    rescue
      UI.error("❌ Launchable authentication failed")
    end
    
    UI.success("✅ Launchable verification complete!")
  end

  desc "Clean build artifacts"
  lane :clean do
    UI.message("🧹 Cleaning build artifacts...")
    
    clear_derived_data
    
    UI.success("✅ Cleanup completed!")
  end

  # Error handling
  error do |lane, exception|
    UI.error("❌ Lane #{lane} failed with error: #{exception.message}")
  end
end

